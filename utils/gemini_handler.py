"""
Gemini Handler para intera√ß√£o com a API do Google Gemini.
Gerencia an√°lise de casos cl√≠nicos e gera√ß√£o de diagn√≥sticos diferenciais.
"""

import streamlit as st
import google.generativeai as genai
import json
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from .logger import get_logger
from .validators import get_friendly_error_message


class GeminiHandler:
    """
    Gerencia intera√ß√µes com a API do Google Gemini para an√°lise m√©dica educacional.
    """
    
    def __init__(self):
        """
        Inicializa o handler e configura a API do Gemini.
        """
        self.model_name = "gemini-2.5-flash"
        self.model = None
        self.logger = get_logger()
        self._configure_api()
    
    def _configure_api(self) -> bool:
        """
        Configura a API do Gemini usando st.secrets.
        
        Returns:
            True se configura√ß√£o foi bem-sucedida, False caso contr√°rio
        """
        try:
            self.logger.info("Iniciando configura√ß√£o da API Gemini")
            
            # Tentar obter API key do st.secrets
            if hasattr(st, 'secrets') and 'GEMINI_API_KEY' in st.secrets:
                api_key = st.secrets['GEMINI_API_KEY']
            else:
                self.logger.error("GEMINI_API_KEY n√£o encontrada em st.secrets")
                st.error("‚ö†Ô∏è GEMINI_API_KEY n√£o encontrada em st.secrets")
                return False
            
            # Configurar API
            genai.configure(api_key=api_key)
            
            # Inicializar modelo
            # Limite real do gemini-2.0-flash-exp: 65.536 tokens de sa√≠da
            # Usando ~50% do limite para balancear qualidade e custo
            self.model = genai.GenerativeModel(
                model_name=self.model_name,
                generation_config={
                    "temperature": 0.7,
                    "top_p": 0.95,
                    "top_k": 40,
                    "max_output_tokens": 32768,  # ~50% do limite (65.536)
                }
            )
            
            self.logger.info(f"API Gemini configurada com sucesso. Modelo: {self.model_name}")
            return True
            
        except Exception as e:
            self.logger.error("Falha ao configurar API Gemini", error=e)
            error_msg = get_friendly_error_message(e)
            st.error(error_msg)
            return False
    
    def _create_diagnostic_prompt(self, context: str, specialty: str) -> str:
        """
        Cria o prompt estruturado para an√°lise diagn√≥stica.
        
        Args:
            context: Contexto completo da consulta
            specialty: Especialidade m√©dica selecionada
        
        Returns:
            Prompt formatado para o modelo
        """
        prompt = f"""Voc√™ √© um assistente m√©dico educacional especializado em {specialty}.

Sua fun√ß√£o √© auxiliar estudantes de medicina e profissionais em forma√ß√£o com an√°lise de casos cl√≠nicos de forma EDUCACIONAL.

IMPORTANTE:
- Esta √© uma ferramenta EDUCACIONAL, n√£o para uso cl√≠nico real
- Forne√ßa diagn√≥sticos diferenciais baseados EXCLUSIVAMENTE nos dados apresentados
- Seja did√°tico e explique o racioc√≠nio cl√≠nico
- Atribua probabilidades realistas baseadas nas informa√ß√µes dispon√≠veis
- Se houver dados insuficientes, indique isso claramente

DADOS DO CASO CL√çNICO:
{context}

ESPECIALIDADE: {specialty}

Por favor, analise este caso e forne√ßa:

1. **Top 5 Diagn√≥sticos Diferenciais** ranqueados por probabilidade
2. Para cada diagn√≥stico, forne√ßa:
   - Nome do diagn√≥stico
   - Probabilidade estimada (0-100%) baseada APENAS nos dados apresentados
   - Justificativa cl√≠nica CONCISA (m√°ximo 3-4 linhas)
   - 3-5 evid√™ncias a favor
   - 1-2 evid√™ncias contra
   - 2-3 dados cl√≠nicos principais

**IMPORTANTE PARA EVITAR TRUNCAMENTO:**
- Justificativas: m√°ximo 250 caracteres cada
- Evid√™ncias: frases curtas e objetivas
- Observa√ß√µes: m√°ximo 150 caracteres
- Dados insuficientes: lista breve

FORMATO DE RESPOSTA (retorne APENAS JSON v√°lido, sem markdown):
{{
  "diagnosticos": [
    {{
      "nome": "Nome do Diagn√≥stico 1",
      "probabilidade": 75,
      "justificativa": "Explica√ß√£o CONCISA do racioc√≠nio cl√≠nico (max 250 chars)",
      "evidencias_favor": ["Evid√™ncia 1", "Evid√™ncia 2", "Evid√™ncia 3"],
      "evidencias_contra": ["Contra 1", "Contra 2"],
      "dados_principais": ["Dado 1", "Dado 2"]
    }},
    {{
      "nome": "Nome do Diagn√≥stico 2",
      "probabilidade": 60,
      "justificativa": "Explica√ß√£o CONCISA (max 250 chars)",
      "evidencias_favor": ["..."],
      "evidencias_contra": ["..."],
      "dados_principais": ["..."]
    }}
  ],
  "observacoes": "Considera√ß√µes importantes BREVES (max 150 chars)",
  "dados_insuficientes": ["Info faltante 1", "Info faltante 2"],
  "nivel_confianca": "alto/m√©dio/baixo"
}}

Retorne SOMENTE o JSON, sem formata√ß√£o markdown, sem ```json, apenas o objeto JSON puro.
SEJA CONCISO para evitar truncamento da resposta.
        
        return prompt
    
    def _parse_json_response(self, response_text: str) -> Optional[Dict]:
        """
        Parseia a resposta JSON do modelo, removendo markdown se necess√°rio.
        Detecta e trata JSON truncado.
        
        Args:
            response_text: Texto da resposta do modelo
        
        Returns:
            Dicion√°rio parseado ou None em caso de erro
        """
        try:
            # Remover poss√≠veis marcadores de markdown
            clean_text = response_text.strip()
            
            # Remover ```json e ``` se presentes
            if clean_text.startswith("```json"):
                clean_text = clean_text[7:]
            elif clean_text.startswith("```"):
                clean_text = clean_text[3:]
            
            if clean_text.endswith("```"):
                clean_text = clean_text[:-3]
            
            clean_text = clean_text.strip()
            
            # Parsear JSON
            return json.loads(clean_text)
            
        except json.JSONDecodeError as e:
            error_msg = str(e)
            
            # Detectar JSON truncado (unterminated string/array/object)
            if "Unterminated" in error_msg or "Expecting" in error_msg:
                self.logger.warning(f"JSON truncado detectado: {error_msg}")
                st.warning(f"‚ö†Ô∏è **Resposta incompleta detectada**")
                st.info("""
                **O que aconteceu?**
                A resposta foi muito longa e foi cortada no meio.
                
                **Solu√ß√£o:**
                1. Clique em "Enviar e Analisar" novamente
                2. O sistema tentar√° gerar uma resposta mais concisa
                3. Se persistir, adicione informa√ß√µes aos poucos
                
                **T√©cnico:** JSON truncado - max_output_tokens atingido
                """)
                
                # Tentar salvar diagn√≥sticos parciais se poss√≠vel
                try:
                    # Tentar completar o JSON minimamente
                    if '"diagnosticos": [' in clean_text:
                        # Fechar arrays e objetos abertos
                        clean_text = clean_text.rstrip(',\n ')
                        # Contar abertura e fechamento
                        open_braces = clean_text.count('{')
                        close_braces = clean_text.count('}')
                        open_brackets = clean_text.count('[')
                        close_brackets = clean_text.count(']')
                        
                        # Adicionar fechamentos necess√°rios
                        for _ in range(open_braces - close_braces):
                            clean_text += '\n}'
                        for _ in range(open_brackets - close_brackets):
                            clean_text += '\n]'
                        
                        # Tentar parsear novamente
                        partial_result = json.loads(clean_text)
                        
                        if 'diagnosticos' in partial_result and partial_result['diagnosticos']:
                            st.success(f"‚úÖ Recuperados {len(partial_result['diagnosticos'])} diagn√≥sticos parciais")
                            return partial_result
                except:
                    pass
            else:
                st.error(f"‚ùå Erro ao parsear resposta JSON: {error_msg}")
            
            # Mostrar JSON problem√°tico em expander para debug
            with st.expander("üîç Ver resposta problem√°tica (Debug)", expanded=False):
                st.code(response_text[:3000] + "..." if len(response_text) > 3000 else response_text, language="text")
            
            return None
            
        except Exception as e:
            self.logger.error("Erro inesperado ao processar resposta", error=e)
            st.error(f"‚ùå Erro inesperado ao processar resposta: {str(e)}")
            return None
    
    def analyze_case(
        self,
        context: str,
        specialty: str,
        max_retries: int = 3
    ) -> Tuple[Optional[Dict], Optional[str]]:
        """
        Analisa um caso cl√≠nico e retorna diagn√≥sticos diferenciais.
        
        Args:
            context: Contexto completo da consulta m√©dica
            specialty: Especialidade m√©dica selecionada
            max_retries: N√∫mero m√°ximo de tentativas em caso de falha
        
        Returns:
            Tupla (resultado_dict, mensagem_erro)
            - resultado_dict: Dicion√°rio com diagn√≥sticos ou None em caso de erro
            - mensagem_erro: Mensagem de erro ou None se bem-sucedido
        """
        start_time = time.time()
        self.logger.info(f"Iniciando an√°lise de caso", specialty=specialty, context_length=len(context))
        
        if not self.model:
            self.logger.error("Modelo Gemini n√£o configurado")
            return None, "Modelo Gemini n√£o est√° configurado corretamente"
        
        if not context or not context.strip():
            self.logger.warning("Contexto vazio fornecido para an√°lise")
            return None, "Contexto vazio fornecido para an√°lise"
        
        # Criar prompt
        prompt = self._create_diagnostic_prompt(context, specialty)
        
        # Tentar an√°lise com retry logic
        for attempt in range(max_retries):
            try:
                self.logger.debug(f"Tentativa {attempt + 1}/{max_retries} de an√°lise")
                
                # Enviar para o modelo
                response = self.model.generate_content(prompt)
                
                # Atualizar timestamp de √∫ltima chamada √† API (rate limiting)
                st.session_state.last_api_call = datetime.now()
                
                # Verificar se h√° resposta v√°lida
                if not response or not response.text:
                    raise ValueError("Resposta vazia do modelo")
                
                # Parsear resposta JSON
                result = self._parse_json_response(response.text)
                
                if result:
                    # Validar estrutura b√°sica
                    if "diagnosticos" in result and isinstance(result["diagnosticos"], list):
                        # Limitar a 5 diagn√≥sticos
                        result["diagnosticos"] = result["diagnosticos"][:5]
                        
                        duration = time.time() - start_time
                        self.logger.log_api_call(
                            endpoint="analyze_case",
                            success=True,
                            duration=duration,
                            num_diagnostics=len(result["diagnosticos"])
                        )
                        
                        return result, None
                    else:
                        raise ValueError("Estrutura JSON inv√°lida na resposta")
                
                # Se chegou aqui, parsing falhou
                if attempt < max_retries - 1:
                    self.logger.warning(f"Falha no parsing, tentando novamente em {2 ** attempt}s")
                    time.sleep(2 ** attempt)  # Backoff exponencial
                    continue
                else:
                    self.logger.error("Falha ao parsear resposta ap√≥s m√∫ltiplas tentativas")
                    return None, "Falha ao parsear resposta ap√≥s m√∫ltiplas tentativas"
                
            except Exception as e:
                self.logger.error(f"Erro na tentativa {attempt + 1}/{max_retries}", error=e)
                error_msg = get_friendly_error_message(e)
                
                if attempt < max_retries - 1:
                    st.warning(f"‚ö†Ô∏è {error_msg} Tentando novamente...")
                    time.sleep(2 ** attempt)  # Backoff exponencial: 1s, 2s, 4s
                else:
                    duration = time.time() - start_time
                    self.logger.log_api_call(
                        endpoint="analyze_case",
                        success=False,
                        duration=duration,
                        error=str(e)
                    )
                    return None, error_msg
        
        return None, "Erro desconhecido durante an√°lise"
    
    def generate_suggestions(
        self,
        context: str,
        specialty: str,
        diagnosticos: Optional[List[Dict]] = None,
        max_retries: int = 2
    ) -> Optional[Dict]:
        """
        Gera sugest√µes de perguntas, exames e condutas baseadas no caso.
        Usa modelo gemini-2.5-flash para respostas mais r√°pidas.
        
        Args:
            context: Contexto da consulta
            specialty: Especialidade m√©dica
            diagnosticos: Lista opcional de diagn√≥sticos diferenciais
            max_retries: N√∫mero m√°ximo de tentativas
        
        Returns:
            Dicion√°rio com sugest√µes ou None em caso de erro
            Formato: {
                "follow_up_questions": [...],
                "suggested_exams": [...],
                "management_suggestions": [...]
            }
        """
        start_time = time.time()
        self.logger.info("Iniciando gera√ß√£o de sugest√µes", specialty=specialty)
        
        try:
            # Criar modelo espec√≠fico para sugest√µes (mais r√°pido)
            suggestions_model = genai.GenerativeModel(
                model_name="gemini-2.0-flash-exp",  # Modelo mais r√°pido para sugest√µes
                generation_config={
                    "temperature": 0.8,
                    "top_p": 0.95,
                    "top_k": 40,
                    "max_output_tokens": 8192,  # Aumentado para sugest√µes mais completas
                }
            )
            
            # Criar contexto adicional se houver diagn√≥sticos
            diagnosticos_context = ""
            if diagnosticos and len(diagnosticos) > 0:
                diagnosticos_str = "\n".join([
                    f"- {d.get('nome', 'N/A')} ({d.get('probabilidade', 0)}%)"
                    for d in diagnosticos[:3]
                ])
                diagnosticos_context = f"""

PRINCIPAIS HIP√ìTESES DIAGN√ìSTICAS CONSIDERADAS:
{diagnosticos_str}"""
            
            prompt = f"""Voc√™ √© um assistente m√©dico educacional especializado em {specialty}.

Com base no caso cl√≠nico abaixo, forne√ßa sugest√µes pr√°ticas e educacionais para auxiliar o estudante na condu√ß√£o adequada do caso.

DADOS DO CASO:
{context}{diagnosticos_context}

Forne√ßa:

1. **PERGUNTAS DE SEGUIMENTO**: 3-5 perguntas ESSENCIAIS e espec√≠ficas a fazer ao paciente para complementar a anamnese e ajudar no diagn√≥stico diferencial. Seja objetivo e pr√°tico.

2. **EXAMES COMPLEMENTARES**: 3-5 exames laboratoriais, de imagem ou outros que sejam PERTINENTES e priorit√°rios para este caso espec√≠fico. Liste do mais urgente/importante para o menos.

3. **SUGEST√ïES DE CONDUTA**: 3-5 recomenda√ß√µes de manejo cl√≠nico BASEADAS NOS DADOS ATUAIS. Inclua medidas iniciais, estabiliza√ß√£o, tratamentos sintom√°ticos ou espec√≠ficos, quando encaminhar, etc.

IMPORTANTE:
- Seja espec√≠fico e relevante para ESTE caso
- Priorize por import√¢ncia cl√≠nica
- Considere a especialidade de {specialty}
- Seja educacional mas pr√°tico

FORMATO DE RESPOSTA (retorne APENAS JSON v√°lido, sem markdown):
{{
  "follow_up_questions": [
    "Pergunta espec√≠fica 1",
    "Pergunta espec√≠fica 2",
    "Pergunta espec√≠fica 3"
  ],
  "suggested_exams": [
    "Exame priorit√°rio 1",
    "Exame priorit√°rio 2",
    "Exame priorit√°rio 3"
  ],
  "management_suggestions": [
    "Conduta/manejo 1",
    "Conduta/manejo 2",
    "Conduta/manejo 3"
  ]
}}

Retorne SOMENTE o objeto JSON, sem formata√ß√£o markdown."""
            
            # Tentar gerar sugest√µes com retry
            for attempt in range(max_retries):
                try:
                    self.logger.debug(f"Tentativa {attempt + 1}/{max_retries} de gerar sugest√µes")
                    response = suggestions_model.generate_content(prompt)
                    
                    # Atualizar timestamp de √∫ltima chamada √† API
                    st.session_state.last_api_call = datetime.now()
                    
                    if response and response.text:
                        result = self._parse_json_response(response.text)
                        
                        if result and self._validate_suggestions_structure(result):
                            duration = time.time() - start_time
                            self.logger.log_api_call(
                                endpoint="generate_suggestions",
                                success=True,
                                duration=duration
                            )
                            return result
                        else:
                            if attempt < max_retries - 1:
                                self.logger.warning("Falha na valida√ß√£o de sugest√µes, tentando novamente")
                                time.sleep(1)
                                continue
                    
                except Exception as e:
                    self.logger.error(f"Erro na tentativa {attempt + 1}/{max_retries} de gerar sugest√µes", error=e)
                    if attempt < max_retries - 1:
                        time.sleep(1)
                        continue
                    else:
                        error_msg = get_friendly_error_message(e)
                        st.warning(f"‚ö†Ô∏è {error_msg}")
                        return None
            
            self.logger.warning("Todas as tentativas de gerar sugest√µes falharam")
            return None
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.log_api_call(
                endpoint="generate_suggestions",
                success=False,
                duration=duration,
                error=str(e)
            )
            error_msg = get_friendly_error_message(e)
            st.error(error_msg)
            return None
    
    def _validate_suggestions_structure(self, data: Dict) -> bool:
        """
        Valida estrutura do JSON de sugest√µes.
        
        Args:
            data: Dicion√°rio a validar
        
        Returns:
            True se v√°lido, False caso contr√°rio
        """
        required_keys = ["follow_up_questions", "suggested_exams", "management_suggestions"]
        
        if not all(key in data for key in required_keys):
            return False
        
        # Verificar se s√£o listas
        for key in required_keys:
            if not isinstance(data[key], list):
                return False
        
        return True
    
    def generate_medical_record(
        self,
        context: str,
        diagnosticos: List[Dict],
        format_type: str = "Tradicional"
    ) -> Optional[str]:
        """
        Gera prontu√°rio m√©dico formatado.
        
        Args:
            context: Contexto da consulta
            diagnosticos: Lista de diagn√≥sticos
            format_type: "Tradicional" ou "SOAP"
        
        Returns:
            Texto do prontu√°rio formatado ou None em caso de erro
        """
        start_time = time.time()
        self.logger.info(f"Iniciando gera√ß√£o de prontu√°rio", format_type=format_type)
        
        if not self.model:
            self.logger.error("Modelo Gemini n√£o configurado")
            return None
        
        diagnosticos_str = "\n".join([
            f"{i+1}. {d['nome']} - {d['probabilidade']}%"
            for i, d in enumerate(diagnosticos[:5])
        ])
        
        if format_type == "SOAP":
            prompt_format = """formato SOAP (Subjetivo, Objetivo, Avalia√ß√£o, Plano):

S (SUBJETIVO): Queixa principal e hist√≥ria
O (OBJETIVO): Exame f√≠sico e dados objetivos
A (AVALIA√á√ÉO): Hip√≥teses diagn√≥sticas e racioc√≠nio
P (PLANO): Condutas e exames propostos"""
        else:
            prompt_format = """formato TRADICIONAL:

- Identifica√ß√£o do paciente
- Queixa principal
- Hist√≥ria da doen√ßa atual
- Antecedentes
- Exame f√≠sico
- Hip√≥teses diagn√≥sticas
- Plano"""
        
        prompt = f"""Crie um prontu√°rio m√©dico EDUCACIONAL em {prompt_format}

DADOS DO CASO:
{context}

HIP√ìTESES DIAGN√ìSTICAS:
{diagnosticos_str}

Gere um prontu√°rio bem estruturado e did√°tico para fins educacionais.
Retorne o texto formatado do prontu√°rio."""
        
        try:
            self.logger.debug("Enviando requisi√ß√£o para gerar prontu√°rio")
            response = self.model.generate_content(prompt)
            
            # Atualizar timestamp de √∫ltima chamada √† API
            st.session_state.last_api_call = datetime.now()
            
            if response and response.text:
                duration = time.time() - start_time
                self.logger.log_api_call(
                    endpoint="generate_medical_record",
                    success=True,
                    duration=duration,
                    format_type=format_type
                )
                return response.text.strip()
            
            self.logger.warning("Resposta vazia ao gerar prontu√°rio")
            return None
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.log_api_call(
                endpoint="generate_medical_record",
                success=False,
                duration=duration,
                error=str(e)
            )
            error_msg = get_friendly_error_message(e)
            st.error(error_msg)
            return None


# Fun√ß√µes utilit√°rias para uso direto

def analyze_case(context: str, specialty: str) -> Tuple[Optional[Dict], Optional[str]]:
    """
    Fun√ß√£o auxiliar para an√°lise de caso.
    
    Args:
        context: Contexto completo da consulta
        specialty: Especialidade m√©dica
    
    Returns:
        Tupla (resultado, erro)
    """
    handler = GeminiHandler()
    return handler.analyze_case(context, specialty)


def get_gemini_handler() -> GeminiHandler:
    """
    Retorna uma inst√¢ncia singleton do GeminiHandler.
    
    Returns:
        Inst√¢ncia do GeminiHandler
    """
    if 'gemini_handler' not in st.session_state:
        st.session_state.gemini_handler = GeminiHandler()
    
    return st.session_state.gemini_handler

